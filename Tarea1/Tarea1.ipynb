{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1\n",
    "\n",
    "En esta tarea vas a programar tu mismo el algoritmo de descenso del gradiente, en dos versiones, y comparar ambos resultados. Para probar tu modelo vas a tener que usar el _dataset_ de gustos de helados que vimos en clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Portada](https://i.postimg.cc/Fz4cHWz6/Copia-de-Iris-Day-in-Belgium-by-Slidesgo-pptx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1 (3 pts): gradient descent simple\n",
    "\n",
    "En el codigo de abajo hay una clase llamada `LogisticRegression` cuyo constructor recibe como parámetro el número de _features_ que espera recibir. Tienes que completar esta clase para que pueda entrenar y predecir. Lo que necesitas es:\n",
    "\n",
    "- Programar el método `train`, que vendría a ser equivalente al método `fit` de Scikit Learn. Tienes que utilizar el algoritmo _Gradient Descent_ visto en clases.\n",
    "- Programar el método `predict` que asume que tu modelo ya está entrenado.\n",
    "\n",
    "Para hacer esto puedes hacer los supuestos razonables que estimes conveniente. Además, si te acomoda trabajar sin clases puedes hacerlo, mientras uses el algoritmo de _Gradient Descent_. **Importante**: puedes asumir que una instancia es \"positiva\" (es decir, pertenece a la clase 1) si la probabilidad calculada es mayor o igual a 0.5.\n",
    "\n",
    "Recuerda además que el gradiente de la función objetivo para cada $\\beta_i$ es:\n",
    "\n",
    "$$\n",
    "\\frac{\\delta}{\\delta \\beta_i}L(\\beta) = \\frac{1}{m} \\sum_{1 \\leq j \\leq m} (\\sigma(\\beta^T x^j) - y_j) x_i^j\n",
    "$$\n",
    "\n",
    "Donde $L(\\beta)$ es la función de verosimilitud, $\\beta$ es el vector de coeficientes para la regresión, tenemos $m$ filas en nuestro _dataset_, $\\sigma(x)$ es la función $\\frac{1}{1 + e^{-x}}$, $x^j$ es la fila $j$ de nuestro dataset (y asociado tiene su respuesta $y_j$) y finalmente $x_i^j$ es la columna $i$ de la fila $j$ en nuestro _dataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35585343]\n",
      " [ 0.90558894]\n",
      " [-0.18238662]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# La función sigmoide\n",
    "def sigmoid(x):    \n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, number_of_features, learning_rate=0.001, number_of_iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.number_of_iterations = number_of_iterations\n",
    "        self.beta = np.random.randn(number_of_features, 1)\n",
    "    #  Train está basado en el fit en: \n",
    "    def train(self, X, y):\n",
    "        self.X = X \n",
    "        self.y = y\n",
    "        for i in range(self.number_of_iterations): \n",
    "            self.cambiando_peso() \n",
    "        return self\n",
    "    \n",
    "    # cambiando_peso es basado en: https://www.geeksforgeeks.org/linear-regression-implementation-from-scratch-using-python/\n",
    "    def cambiando_peso(self):\n",
    "        y_predecido = self.predict(self.X)\n",
    "        error = y_predecido - self.y.values.reshape(-1, 1)\n",
    "        dW = (self.X.T).dot(error) / self.X.shape[0]\n",
    "        self.beta = self.beta - self.learning_rate * dW\n",
    "    # Predict está basado en: https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac \n",
    "    def predict(self, value):\n",
    "        probabilidad = sigmoid(value.dot(self.beta))\n",
    "        return np.where(probabilidad >= 0.5, 1, 0)\n",
    "\n",
    "# Ejemplo de uso de la clase para 3 features\n",
    "log_reg = LogisticRegression(3)\n",
    "print(log_reg.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 (1 pto): entrenando el modelo, computando el log loss\n",
    "\n",
    "En esta parte tendrás que hacer un clasificador de gustos de helados tal como lo hicimos para regresión logística. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   Unnamed: 0  200 non-null    int64\n",
      " 1   id          200 non-null    int64\n",
      " 2   female      200 non-null    int64\n",
      " 3   ice_cream   200 non-null    int64\n",
      " 4   video       200 non-null    int64\n",
      " 5   puzzle      200 non-null    int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 9.5 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "helados = pd.read_csv('Ice_cream.csv')\n",
    "helados.info()\n",
    "X = helados[['female', 'video', 'puzzle']]\n",
    "y = helados['ice_cream']\n",
    "\n",
    "\n",
    "# Entrena el modelo acá, debería ser como la siguiente línea\n",
    "log_reg.train(X, y)\n",
    "\n",
    "# Ahora usa el predict\n",
    "predicciones = log_reg.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En este caso el log loss es de: 16.580080558993888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "#Obtiene el log loss\n",
    "manual = log_loss(helados['ice_cream'], predicciones)\n",
    "print(f\"En este caso el log loss es de: {manual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si usamos la librería el log loss es de: 16.03942575815713. \n",
      "Es una diferencia de 0.5406548008367587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "reg = LogisticRegression()\n",
    "\n",
    "reg.fit(X, y)\n",
    "result= reg.predict(X)\n",
    "\n",
    "libr = log_loss(helados['ice_cream'], result)\n",
    "print(f\"Si usamos la librería el log loss es de: {libr}. \\nEs una diferencia de {abs(manual - libr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la diferencia de hacerlo entre mi código y la librería SkLearn es mínima. Con esto comprobamos que es un buen modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3 (1.5 ptos) mini-batch gradient descent\n",
    "\n",
    "Otra forma de implementar el _gradient descent_ es mediante batches, o pedazos. La idea es la siguiente: \n",
    "- Se selecciona un número B de pedazos a usar y un número E de épocas\n",
    "- Para cada época: \n",
    "    - Dividir el set de entrenamiento en B pedazos \n",
    "    - Para cada pedazo: \n",
    "        - calcular el gradiente usando la fórmula, pero solo para los datos de ese pedazo. \n",
    "        - realizar una iteración del gradient descent y actualizar los coeficientes\n",
    "        \n",
    "En esta última parte, modifica tu clase LogisticRegression para que el train sea hecho con mini-bath gradient descent. Las épocas y los batches deben ser pasados como parámetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tienes que programar la parte 3 aquí\n",
    "import numpy as np\n",
    "\n",
    "# La función sigmoide\n",
    "def sigmoid(x):    \n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "class LogisticRegressionBatches:\n",
    "    def __init__(self, number_of_features,n_batches,n_epochs, learning_rate=0.001):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_batches = n_batches\n",
    "        self.n_epochs = n_epochs\n",
    "        self.beta = np.random.randn(number_of_features, 1)\n",
    "\n",
    "    # Basado en la respuesta disponible en: https://stackoverflow.com/questions/45113245/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way\n",
    "    def batchify(self, X, y, batch_size):\n",
    "        m = X.shape[0]\n",
    "        batches = []\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            batches.append((X_batch, y_batch))\n",
    "        return batches\n",
    "  \n",
    "    def train(self, X, y):\n",
    "        batch_size = X.shape[0] // self.n_batches\n",
    "        for _ in range(self.n_epochs):\n",
    "            batches = self.batchify(X, y, batch_size)\n",
    "            for X_batch, y_batch in batches:\n",
    "                self.cambiando_peso(X_batch, y_batch)\n",
    "    \n",
    "    def cambiando_peso(self, X, y):\n",
    "        y_predecido = self.predict(X).flatten()\n",
    "        error = y_predecido - y\n",
    "        dW = (X.T).dot(error.values.reshape(-1, 1)) / X.shape[0]\n",
    "        self.beta = self.beta - self.learning_rate * dW\n",
    "        return self\n",
    "    \n",
    "    def predict(self, value):\n",
    "        probabilidad = sigmoid(value.dot(self.beta))\n",
    "        return np.where(probabilidad >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4 (0.5 pts): número de batches y épocas?\n",
    "\n",
    "Lo último que debes hacer es encontrar un número de batches y épocas de forma que el modelo que entrenas con esos batches y épocas entregue un log-loss similar al que obtuviste con el método de gradient descent en la parte 1-2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.120735359830647"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trabaja acá\n",
    "#Probaré primero con una cantidad 3 para comprobar que funciona. \n",
    "batch_log = LogisticRegressionBatches(3,3,3)\n",
    "\n",
    "X_parte2 = helados[['female', 'video', 'puzzle']]\n",
    "y_parte2 = helados['ice_cream']\n",
    "\n",
    "\n",
    "# Entrena el modelo acá, debería ser como la siguiente línea\n",
    "batch_log.train(X_parte2, y_parte2)\n",
    "\n",
    "# Ahora usa el predict\n",
    "batch_predicciones = batch_log.predict(X_parte2)\n",
    "\n",
    "log_loss(helados['ice_cream'], batch_predicciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crearé la siguiente función para probar varios valores. \n",
    "def tryingBatch(target):\n",
    "    target_log_loss = target\n",
    "    best_diff = float('inf')\n",
    "    best_batch = None\n",
    "    best_epoch = None\n",
    "\n",
    "    for batch in range(1, 10):  \n",
    "        for epoch in range(1, 10):  \n",
    "            new_batch_log = LogisticRegressionBatches(3, batch, epoch)\n",
    "            X_3 = helados[['female', 'video', 'puzzle']]\n",
    "            y_3 = helados['ice_cream']\n",
    "            new_batch_log.train(X_3, y_3)\n",
    "            batch_predicciones = new_batch_log.predict(X_3)\n",
    "            log_loss_value = log_loss(y_3, batch_predicciones)\n",
    "            diff = abs(log_loss_value - target_log_loss)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_batch = batch\n",
    "                best_epoch = epoch\n",
    "\n",
    "    print(f\"El log_loss más cercano se consigue con: {best_batch} batches y {best_epoch} epocas.\\nEl valor es de {log_loss_value:.6f} con una diferencia de {abs(target - log_loss_value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de la parte 2 es: 16.580080558993888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El log_loss más cercano se consigue con: 3 batches y 4 epocas.\n",
      "El valor es de 18.021827 con una diferencia de 1.441746135564685\n"
     ]
    }
   ],
   "source": [
    "print(f\"El valor de la parte 2 es: {manual}\")\n",
    "tryingBatch(manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detalles académicos\n",
    "\n",
    "La entrega de este control debe ser un solo archivo **Jupyter Notebook**. **La fecha de entrega es hasta el viernes 30 de Agosto, hasta las 20:00 pm**. La nota se calcula como el número de puntos + un punto base. El archivo se entrega en un cuestionario en canvas. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
