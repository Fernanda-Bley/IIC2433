{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1\n",
    "\n",
    "En esta tarea vas a programar tu mismo el algoritmo de descenso del gradiente, en dos versiones, y comparar ambos resultados. Para probar tu modelo vas a tener que usar el _dataset_ de gustos de helados que vimos en clases.\n",
    "\n",
    "## Parte 1 (3 pts): gradient descent simple\n",
    "\n",
    "En el codigo de abajo hay una clase llamada `LogisticRegression` cuyo constructor recibe como parámetro el número de _features_ que espera recibir. Tienes que completar esta clase para que pueda entrenar y predecir. Lo que necesitas es:\n",
    "\n",
    "- Programar el método `train`, que vendría a ser equivalente al método `fit` de Scikit Learn. Tienes que utilizar el algoritmo _Gradient Descent_ visto en clases.\n",
    "- Programar el método `predict` que asume que tu modelo ya está entrenado.\n",
    "\n",
    "Para hacer esto puedes hacer los supuestos razonables que estimes conveniente. Además, si te acomoda trabajar sin clases puedes hacerlo, mientras uses el algoritmo de _Gradient Descent_. **Importante**: puedes asumir que una instancia es \"positiva\" (es decir, pertenece a la clase 1) si la probabilidad calculada es mayor o igual a 0.5.\n",
    "\n",
    "Recuerda además que el gradiente de la función objetivo para cada $\\beta_i$ es:\n",
    "\n",
    "$$\n",
    "\\frac{\\delta}{\\delta \\beta_i}L(\\beta) = \\frac{1}{m} \\sum_{1 \\leq j \\leq m} (\\sigma(\\beta^T x^j) - y_j) x_i^j\n",
    "$$\n",
    "\n",
    "Donde $L(\\beta)$ es la función de verosimilitud, $\\beta$ es el vector de coeficientes para la regresión, tenemos $m$ filas en nuestro _dataset_, $\\sigma(x)$ es la función $\\frac{1}{1 + e^{-x}}$, $x^j$ es la fila $j$ de nuestro dataset (y asociado tiene su respuesta $y_j$) y finalmente $x_i^j$ es la columna $i$ de la fila $j$ en nuestro _dataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20594365]\n",
      " [-1.32955507]\n",
      " [ 0.56162231]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# La función sigmoide\n",
    "def sigmoid(x):    \n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, number_of_features, learning_rate=0.001, number_of_iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.number_of_iterations = number_of_iterations\n",
    "        self.beta = np.random.randn(number_of_features, 1)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        self.X = X \n",
    "        self.y = y\n",
    "        \n",
    "        for i in range(self.number_of_iterations): \n",
    "            self.cambiando_peso() \n",
    "        return self\n",
    "    \n",
    "    def cambiando_peso(self):\n",
    "        y_predecido = self.predict(self.X)\n",
    "        error = y_predecido - self.y.values.reshape(-1, 1)\n",
    "        dW = (self.X.T).dot(error) / self.X.shape[0]\n",
    "        self.beta = self.beta - self.learning_rate * dW\n",
    "    \n",
    "    def predict(self, value):\n",
    "        probabilidad = sigmoid(value.dot(self.beta))\n",
    "        return np.where(probabilidad >= 0.5, 1, 0)\n",
    "\n",
    "# Ejemplo de uso de la clase para 3 features\n",
    "log_reg = LogisticRegression(3)\n",
    "print(log_reg.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 (1 pto): entrenando el modelo, computando el log loss\n",
    "\n",
    "En esta parte tendrás que hacer un clasificador de gustos de helados tal como lo hicimos para regresión logística. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   Unnamed: 0  200 non-null    int64\n",
      " 1   id          200 non-null    int64\n",
      " 2   female      200 non-null    int64\n",
      " 3   ice_cream   200 non-null    int64\n",
      " 4   video       200 non-null    int64\n",
      " 5   puzzle      200 non-null    int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 9.5 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "helados = pd.read_csv('Ice_cream.csv')\n",
    "helados.info()\n",
    "X = helados[['female', 'video', 'puzzle']]\n",
    "y = helados['ice_cream']\n",
    "\n",
    "\n",
    "# Entrena el modelo acá, debería ser como la siguiente línea\n",
    "log_reg.train(X, y)\n",
    "\n",
    "# Ahora usa el predict\n",
    "predicciones = log_reg.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.463572830123262"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "#Obtiene el log loss\n",
    "log_loss(helados['ice_cream'], predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3 (1.5 ptos) mini-batch gradient descent\n",
    "\n",
    "Otra forma de implementar el _gradient descent_ es mediante batches, o pedazos. La idea es la siguiente: \n",
    "- Se selecciona un número B de pedazos a usar y un número E de épocas\n",
    "- Para cada época: \n",
    "    - Dividir el set de entrenamiento en B pedazos \n",
    "    - Para cada pedazo: \n",
    "        - calcular el gradiente usando la fórmula, pero solo para los datos de ese pedazo. \n",
    "        - realizar una iteración del gradient descent y actualizar los coeficientes\n",
    "        \n",
    "En esta última parte, modifica tu clase LogisticRegression para que el train sea hecho con mini-bath gradient descent. Las épocas y los batches deben ser pasados como parámetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tienes que programar la parte 3 aquí\n",
    "import numpy as np\n",
    "\n",
    "# La función sigmoide\n",
    "def sigmoid(x):    \n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "class LogisticRegressionBatches:\n",
    "    def __init__(self, number_of_features,n_batches,n_epochs, learning_rate=0.001):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_batches = n_batches\n",
    "        self.n_epochs = n_epochs\n",
    "        self.beta = np.random.randn(number_of_features, 1)\n",
    "\n",
    "    # Basado en la respuesta disponible en: https://stackoverflow.com/questions/45113245/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way\n",
    "    def batchify(X, y, batch_size):\n",
    "        m = X.shape[0]\n",
    "        batches = []\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            batches.append((X_batch, y_batch))\n",
    "        return batches\n",
    "  \n",
    "    def train(self, X, y):\n",
    "        # Tienes que programar este método\n",
    "        batches = batchify(X, y, self.n_batches)\n",
    "        for epoch in self.n_epochs:\n",
    "            for X_batch, y_batch in batches:\n",
    "                self.cambiando_peso(X_batch, y_batch)\n",
    "    \n",
    "    def cambiando_peso(self, X, y):\n",
    "        \n",
    "        y_predecido = self.predict(X).flatten()\n",
    "        error = y_predecido - y\n",
    "        dW = (X.T).dot(error.values.reshape(-1, 1)) / X.shape[0]\n",
    "        self.beta = self.beta - self.learning_rate * dW\n",
    "        return self\n",
    "    \n",
    "    def predict(self, value):\n",
    "        # Tienes que programar este método\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4 (0.5 pts): número de batches y épocas?\n",
    "\n",
    "Lo último que debes hacer es encontrar un número de batches y épocas de forma que el modelo que entrenas con esos batches y épocas entregue un log-loss similar al que obtuviste con el método de gradient descent en la parte 1-2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.463572830123262"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trabaja acá\n",
    "\n",
    "LogisticRegressionBatches(3,3,3)\n",
    "\n",
    "X = helados[['female', 'video', 'puzzle']]\n",
    "y = helados['ice_cream']\n",
    "\n",
    "\n",
    "# Entrena el modelo acá, debería ser como la siguiente línea\n",
    "log_reg.train(X, y)\n",
    "\n",
    "# Ahora usa el predict\n",
    "predicciones = log_reg.predict(X)\n",
    "\n",
    "log_loss(helados['ice_cream'], predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detalles académicos\n",
    "\n",
    "La entrega de este control debe ser un solo archivo **Jupyter Notebook**. **La fecha de entrega es hasta el viernes 30 de Agosto, hasta las 20:00 pm**. La nota se calcula como el número de puntos + un punto base. El archivo se entrega en un cuestionario en canvas. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
