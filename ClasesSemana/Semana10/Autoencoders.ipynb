{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789c6d0d",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "Los autoencoders son redes neuronales que aprenden sobre un mismo set de datos, y pueden usarse para reducir su dimensionalidad. Esta semana seguimos trabajando con el dataset de MNIST.  Vamos a ver como aprender y cómo usar estas redes, y el ejercicio abierto es que veas la capacidad de reducir dimensionalidad para visualizar.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f21cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1db537e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, as_frame=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b7fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Tomamos el dataset y lo dividimos en X (los pixeles) e y (el número que es). \n",
    "#Recordemos que las imágenes son 28x28 = 784 pixeles\n",
    "X, y = mnist['data'], mnist['target']\n",
    "X_sample = X[60000:]\n",
    "y_sample = y[60000:]\n",
    "\n",
    "### Para dividir en train  - test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75260e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9cb810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Si bien los datos no están completamente balanceados, hay una buena mezcla de cada número en el dataset.\n",
    "y_sample.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "203d29bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 784 artists>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAP3RFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMS5wb3N0MSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8kixA/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwYElEQVR4nO3dfVhVdb7//xeI3HgDiA4gEypTHe810yLKOjVyiUZNluPJonKK9NTAyZsuTRsj04qyNG9y9FipdYWjOVd6zAwlLKlEVJRUMrKTk55sw5xjsNMSVD6/P/qyfu7EG3Rz99nPx3Wt64L1ee+1Pp+19l7rxdprb/yMMUYAAACW8W/sDgAAANQHQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEoBjd2BxlRdXa3Dhw+rbdu28vPza+zuAACAC2CM0Y8//qiYmBj5+5/9eo1Ph5zDhw8rNja2sbsBAAAuwqFDh3TZZZedtd2nQ07btm0l/bKRQkNDG7k3AADgQrjdbsXGxjrn8bPx6ZBT8xZVaGgoIQcAgGbmfLeacOMxAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQAwAArETIAQAAViLkAPBpXSa/39hdAFBPCDkA0AgIV0D9I+QAwEUgpABNHyEHAOBzCKm+gZADAI2Iky1Qfwg58FmcXADAboQcAM1Wl8nvE1YBnBUhB2gkF3ty5qQO2zT0c5rXkO8g5ACNwJsHWQ7YF4ftdukaaxuy73ChCDkAgAbXVIJKU+kH6gchB2hgHFQ9sT0uDtsNOD9CDtDIfPlk5ctj95bmvA2bc9/RPBBy4JM4uDYetj28gecRLgQhB0CdcHIB0FwQcoBLxEkfzRXfMwTbEXKAi8CJATa5lOdzY78WGnv9aNoIOQAAwEqEHKAR8VcoANQfQg6AS8JX8qOxNPZzobHXj/Orc8jJy8vT7bffrpiYGPn5+WnNmjVO24kTJ/TEE0+od+/eat26tWJiYvTAAw/o8OHDHss4cuSIUlJSFBoaqvDwcKWmpuro0aMeNbt379aNN96o4OBgxcbGaubMmWf0ZdWqVerWrZuCg4PVu3dvrV+/vq7DAazAwRbNCc9XNJQ6h5xjx46pb9++WrBgwRltP/30k3bu3KmnnnpKO3fu1LvvvquSkhL94Q9/8KhLSUlRcXGxcnJytG7dOuXl5WnMmDFOu9vt1uDBg9W5c2cVFhbqpZde0rRp07R48WKnZsuWLbrnnnuUmpqqXbt2adiwYRo2bJj27t1b1yEBF42DNSSeB0BTFVDXBwwdOlRDhw6ttS0sLEw5OTke81599VVde+21OnjwoDp16qR9+/YpOztb27dv14ABAyRJ8+fP16233qqXX35ZMTExysrKUlVVlZYsWaLAwED17NlTRUVFmj17thOG5s6dqyFDhmjixImSpBkzZignJ0evvvqqFi1aVNdhAfBBhBPAbvV+T05FRYX8/PwUHh4uScrPz1d4eLgTcCQpMTFR/v7+KigocGpuuukmBQYGOjVJSUkqKSnRDz/84NQkJiZ6rCspKUn5+fln7UtlZaXcbrfHhObL109Qvj7+i8V2A3xHvYac48eP64knntA999yj0NBQSZLL5VJkZKRHXUBAgCIiIuRyuZyaqKgoj5qa389XU9Nem8zMTIWFhTlTbGzspQ0QVuCkBwB2qreQc+LECf3bv/2bjDFauHBhfa2mTqZMmaKKigpnOnToUGN3CfBZhEvvYVsCtauXkFMTcL799lvl5OQ4V3EkKTo6WmVlZR71J0+e1JEjRxQdHe3UlJaWetTU/H6+mpr22gQFBSk0NNRjApozTm5oLE3pudeU+oKmxeshpybg7N+/Xx9++KHat2/v0Z6QkKDy8nIVFhY68zZt2qTq6mrFx8c7NXl5eTpx4oRTk5OTo65du6pdu3ZOTW5urseyc3JylJCQ4O0hAUCtOLkCTVudQ87Ro0dVVFSkoqIiSdKBAwdUVFSkgwcP6sSJE/rjH/+oHTt2KCsrS6dOnZLL5ZLL5VJVVZUkqXv37hoyZIhGjx6tbdu26bPPPlN6erpGjhypmJgYSdK9996rwMBApaamqri4WCtXrtTcuXM1YcIEpx9jx45Vdna2Zs2apS+//FLTpk3Tjh07lJ6e7oXNAtjpbCdlTtaNi+0P1I86h5wdO3aoX79+6tevnyRpwoQJ6tevnzIyMvTdd99p7dq1+p//+R9dddVV6tixozNt2bLFWUZWVpa6deumQYMG6dZbb9XAgQM9vgMnLCxMGzdu1IEDB9S/f389/vjjysjI8Pguneuvv17Lly/X4sWL1bdvX/3973/XmjVr1KtXr0vZHgAAwBJ1/p6cm2++WcaYs7afq61GRESEli9ffs6aPn366JNPPjlnzYgRIzRixIjzrg8AAPge/ncVAACwEiEHAABYiZAD1BE3iYLnANA8EHIAAICVCDkAAMBKhBzAh3nrbRfevgHQFBFyAACAlQg5gA/iykvj8uXt78tjR8Mj5ADNCCcIALhwhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAHQ7HADNoALQcgB0OgILc0f+xBNESEHAABYiZADAACsRMgBgGbmUt8a4q0l+ApCDpo1Dta+xYb9bcMYgOaCkINmiRMFAOB8CDkAAFwk/uBq2gg5AJq9Cz3RcEICfAshBwAAWImQAwAArETIAQAAViLkAAAAKxFyAAAXjJu30ZwQcgA0mOZ+gmzu/Qd8DSEHPocT1S/YDsDF4bXTfBByAACAlQg5AOqMfxAJoDkg5AC4KAQVAE0dIQcAmhHCJXDhCDkAAMBKhBygAfFXOAA0HEIOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAwIfwCT/4EkIOAACwEiEHABoIV1GAhlXnkJOXl6fbb79dMTEx8vPz05o1azzajTHKyMhQx44dFRISosTERO3fv9+j5siRI0pJSVFoaKjCw8OVmpqqo0ePetTs3r1bN954o4KDgxUbG6uZM2ee0ZdVq1apW7duCg4OVu/evbV+/fq6DgcAAFiqziHn2LFj6tu3rxYsWFBr+8yZMzVv3jwtWrRIBQUFat26tZKSknT8+HGnJiUlRcXFxcrJydG6deuUl5enMWPGOO1ut1uDBw9W586dVVhYqJdeeknTpk3T4sWLnZotW7bonnvuUWpqqnbt2qVhw4Zp2LBh2rt3b12HBAAALFTnkDN06FA9++yzuvPOO89oM8Zozpw5mjp1qu644w716dNHb731lg4fPuxc8dm3b5+ys7P1+uuvKz4+XgMHDtT8+fO1YsUKHT58WJKUlZWlqqoqLVmyRD179tTIkSP12GOPafbs2c665s6dqyFDhmjixInq3r27ZsyYoauvvlqvvvrqRW4KAIAv4m1Ee3n1npwDBw7I5XIpMTHRmRcWFqb4+Hjl5+dLkvLz8xUeHq4BAwY4NYmJifL391dBQYFTc9NNNykwMNCpSUpKUklJiX744Qen5vT11NTUrKc2lZWVcrvdHhMAALCTV0OOy+WSJEVFRXnMj4qKctpcLpciIyM92gMCAhQREeFRU9syTl/H2Wpq2muTmZmpsLAwZ4qNja3rEIFa8ZcgADQ9PvXpqilTpqiiosKZDh061NhdAgAA9cSrISc6OlqSVFpa6jG/tLTUaYuOjlZZWZlH+8mTJ3XkyBGPmtqWcfo6zlZT016boKAghYaGekwAAMBOXg05cXFxio6OVm5urjPP7XaroKBACQkJkqSEhASVl5ersLDQqdm0aZOqq6sVHx/v1OTl5enEiRNOTU5Ojrp27ap27do5Naevp6amZj0AAMC31TnkHD16VEVFRSoqKpL0y83GRUVFOnjwoPz8/DRu3Dg9++yzWrt2rfbs2aMHHnhAMTExGjZsmCSpe/fuGjJkiEaPHq1t27bps88+U3p6ukaOHKmYmBhJ0r333qvAwEClpqaquLhYK1eu1Ny5czVhwgSnH2PHjlV2drZmzZqlL7/8UtOmTdOOHTuUnp5+6VsFAJqZ5n5fWHPvP5qmgLo+YMeOHbrllluc32uCx6hRo7Rs2TJNmjRJx44d05gxY1ReXq6BAwcqOztbwcHBzmOysrKUnp6uQYMGyd/fX8OHD9e8efOc9rCwMG3cuFFpaWnq37+/OnTooIyMDI/v0rn++uu1fPlyTZ06VU8++aSuvPJKrVmzRr169bqoDQEAAOxS55Bz8803yxhz1nY/Pz9Nnz5d06dPP2tNRESEli9ffs719OnTR5988sk5a0aMGKERI0acu8MAAMAn+dSnqwCgqeLtGsD7CDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEqEHACAz+CbpX0LIQcAAFiJkAMAAKxEyAEAL+BtEKDpIeQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEA8T03gI0IOQAAwEqEHAAAYCVCDgAAsBIhBwAAWImQA9QBN6cCQPNByAFgBQJo08c+QkMj5AAAACsRcgAAgJUIOQAAwEqEHADABeGeGjQ3hBwAAGAlQg4AALASIQcQl+EBwEaEHAAAYCVCDgAAsBIhBwAAWImQAwBoFrh3DnVFyAEAAFYi5AAAACsRcgAAaAS8/Vb/CDmAJThgAoAnr4ecU6dO6amnnlJcXJxCQkJ0+eWXa8aMGTLGODXGGGVkZKhjx44KCQlRYmKi9u/f77GcI0eOKCUlRaGhoQoPD1dqaqqOHj3qUbN7927deOONCg4OVmxsrGbOnOnt4QCANQjC8DVeDzkvvviiFi5cqFdffVX79u3Tiy++qJkzZ2r+/PlOzcyZMzVv3jwtWrRIBQUFat26tZKSknT8+HGnJiUlRcXFxcrJydG6deuUl5enMWPGOO1ut1uDBw9W586dVVhYqJdeeknTpk3T4sWLvT0koEFwAgIA7wrw9gK3bNmiO+64Q8nJyZKkLl266G9/+5u2bdsm6ZerOHPmzNHUqVN1xx13SJLeeustRUVFac2aNRo5cqT27dun7Oxsbd++XQMGDJAkzZ8/X7feeqtefvllxcTEKCsrS1VVVVqyZIkCAwPVs2dPFRUVafbs2R5hCAAA+CavX8m5/vrrlZubq6+++kqS9Pnnn+vTTz/V0KFDJUkHDhyQy+VSYmKi85iwsDDFx8crPz9fkpSfn6/w8HAn4EhSYmKi/P39VVBQ4NTcdNNNCgwMdGqSkpJUUlKiH374oda+VVZWyu12e0wAAMBOXr+SM3nyZLndbnXr1k0tWrTQqVOn9NxzzyklJUWS5HK5JElRUVEej4uKinLaXC6XIiMjPTsaEKCIiAiPmri4uDOWUdPWrl27M/qWmZmpZ555xgujBAAATZ3Xr+S88847ysrK0vLly7Vz5069+eabevnll/Xmm296e1V1NmXKFFVUVDjToUOHGrtLAACgnnj9Ss7EiRM1efJkjRw5UpLUu3dvffvtt8rMzNSoUaMUHR0tSSotLVXHjh2dx5WWluqqq66SJEVHR6usrMxjuSdPntSRI0ecx0dHR6u0tNSjpub3mppfCwoKUlBQ0KUPEgAANHlev5Lz008/yd/fc7EtWrRQdXW1JCkuLk7R0dHKzc112t1utwoKCpSQkCBJSkhIUHl5uQoLC52aTZs2qbq6WvHx8U5NXl6eTpw44dTk5OSoa9eutb5VhaaHTxMBAOqT10PO7bffrueee07vv/++/vGPf2j16tWaPXu27rzzTkmSn5+fxo0bp2effVZr167Vnj179MADDygmJkbDhg2TJHXv3l1DhgzR6NGjtW3bNn322WdKT0/XyJEjFRMTI0m69957FRgYqNTUVBUXF2vlypWaO3euJkyY4O0hAQCAZsjrb1fNnz9fTz31lP785z+rrKxMMTEx+vd//3dlZGQ4NZMmTdKxY8c0ZswYlZeXa+DAgcrOzlZwcLBTk5WVpfT0dA0aNEj+/v4aPny45s2b57SHhYVp48aNSktLU//+/dWhQwdlZGTw8XEAACCpHkJO27ZtNWfOHM2ZM+esNX5+fpo+fbqmT59+1pqIiAgtX778nOvq06ePPvnkk4vtKgAAsBj/uwoAAFiJkANAEjeCA7APIQdAgyBEAWhohBwAQLNHiEZtCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgDAEnxXDOCJkAMAAKxEyAEAoIFx1a1hEHIAAICVCDkAAMBKhBwAAGAlQg4AALASIQeAV3AjJYCmhpADAACsRMgBAOAScBWz6SLkAAAAKxFyAFwQ/loF0NwQcgAAgJUIOQAAwEqEHAAAYCVCDgBAEvddNQa2ef0i5AAAACsRcgAAgJUIOQAAwEqEHAAAYCVCDhoFN9sBAOobIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAuEB8aKJ5IeSgUXHAAADUF0IO4CMIlAB8DSEHAABYiZADAACsRMgBAABWqpeQ89133+m+++5T+/btFRISot69e2vHjh1OuzFGGRkZ6tixo0JCQpSYmKj9+/d7LOPIkSNKSUlRaGiowsPDlZqaqqNHj3rU7N69WzfeeKOCg4MVGxurmTNn1sdwAABAM+T1kPPDDz/ohhtuUMuWLfXBBx/oiy++0KxZs9SuXTunZubMmZo3b54WLVqkgoICtW7dWklJSTp+/LhTk5KSouLiYuXk5GjdunXKy8vTmDFjnHa3263Bgwerc+fOKiws1EsvvaRp06Zp8eLF3h4SAABohgK8vcAXX3xRsbGxWrp0qTMvLi7O+dkYozlz5mjq1Km64447JElvvfWWoqKitGbNGo0cOVL79u1Tdna2tm/frgEDBkiS5s+fr1tvvVUvv/yyYmJilJWVpaqqKi1ZskSBgYHq2bOnioqKNHv2bI8wBLt0mfy+/vFCcmN3AwDQDHj9Ss7atWs1YMAAjRgxQpGRkerXr59ee+01p/3AgQNyuVxKTEx05oWFhSk+Pl75+fmSpPz8fIWHhzsBR5ISExPl7++vgoICp+amm25SYGCgU5OUlKSSkhL98MMP3h4WAABewdc5NByvh5xvvvlGCxcu1JVXXqkNGzbo0Ucf1WOPPaY333xTkuRyuSRJUVFRHo+Liopy2lwulyIjIz3aAwICFBER4VFT2zJOX8evVVZWyu12e0wAAMBOXn+7qrq6WgMGDNDzzz8vSerXr5/27t2rRYsWadSoUd5eXZ1kZmbqmWeeadQ+AACAhuH1KzkdO3ZUjx49POZ1795dBw8elCRFR0dLkkpLSz1qSktLnbbo6GiVlZV5tJ88eVJHjhzxqKltGaev49emTJmiiooKZzp06NDFDBEAADQDXg85N9xwg0pKSjzmffXVV+rcubOkX25Cjo6OVm5urtPudrtVUFCghIQESVJCQoLKy8tVWFjo1GzatEnV1dWKj493avLy8nTixAmnJicnR127dvX4JNfpgoKCFBoa6jEBAAA7eT3kjB8/Xlu3btXzzz+vr7/+WsuXL9fixYuVlpYmSfLz89O4ceP07LPPau3atdqzZ48eeOABxcTEaNiwYZJ+ufIzZMgQjR49Wtu2bdNnn32m9PR0jRw5UjExMZKke++9V4GBgUpNTVVxcbFWrlypuXPnasKECd4eEtAkcLMiANSN1+/Jueaaa7R69WpNmTJF06dPV1xcnObMmaOUlBSnZtKkSTp27JjGjBmj8vJyDRw4UNnZ2QoODnZqsrKylJ6erkGDBsnf31/Dhw/XvHnznPawsDBt3LhRaWlp6t+/vzp06KCMjAw+Pg4AACTVQ8iRpNtuu0233XbbWdv9/Pw0ffp0TZ8+/aw1ERERWr58+TnX06dPH33yyScX3U8AgPfwPVZoavjfVQAAwEqEHFiB+1VgO57jQN0RcgAAgJUIOQAAwEqEHABoALzdBDQ8Qg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5KDBcQMmmiOet0DzQ8gBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQeAT+Aj4IDvIeQAAAArEXIAAICVCDkA4CW8JQY0LYQcAABgJUIOAACwEiEHAODzeKvRToQcAPh/ONEBdiHkAAAAKxFyAACAlQg5AADASoQcAABgJUIOAACwEiEHgDX4dBSA0xFyAACAlQg5AADASoQcAABgJUIOgCaDe2oAeBMhBwAAWImQAwAArETIgc/grRAA8C2EHAAAYCVCDgCgSeBqK7yNkAMAAKxEyAEAAFYi5AAAACsRcgAAgJXqPeS88MIL8vPz07hx45x5x48fV1pamtq3b682bdpo+PDhKi0t9XjcwYMHlZycrFatWikyMlITJ07UyZMnPWo+/vhjXX311QoKCtIVV1yhZcuW1fdwAABAM1GvIWf79u36z//8T/Xp08dj/vjx4/Xee+9p1apV2rx5sw4fPqy77rrLaT916pSSk5NVVVWlLVu26M0339SyZcuUkZHh1Bw4cEDJycm65ZZbVFRUpHHjxunhhx/Whg0b6nNIAACgmai3kHP06FGlpKTotddeU7t27Zz5FRUVeuONNzR79mz9/ve/V//+/bV06VJt2bJFW7dulSRt3LhRX3zxhd5++21dddVVGjp0qGbMmKEFCxaoqqpKkrRo0SLFxcVp1qxZ6t69u9LT0/XHP/5Rr7zySn0NCQAANCP1FnLS0tKUnJysxMREj/mFhYU6ceKEx/xu3bqpU6dOys/PlyTl5+erd+/eioqKcmqSkpLkdrtVXFzs1Px62UlJSc4yAACAbwuoj4WuWLFCO3fu1Pbt289oc7lcCgwMVHh4uMf8qKgouVwup+b0gFPTXtN2rhq3262ff/5ZISEhZ6y7srJSlZWVzu9ut7vugwMAAM2C16/kHDp0SGPHjlVWVpaCg4O9vfhLkpmZqbCwMGeKjY1t7C4BAIB64vWQU1hYqLKyMl199dUKCAhQQECANm/erHnz5ikgIEBRUVGqqqpSeXm5x+NKS0sVHR0tSYqOjj7j01Y1v5+vJjQ0tNarOJI0ZcoUVVRUONOhQ4e8MWQAANAEeT3kDBo0SHv27FFRUZEzDRgwQCkpKc7PLVu2VG5urvOYkpISHTx4UAkJCZKkhIQE7dmzR2VlZU5NTk6OQkND1aNHD6fm9GXU1NQsozZBQUEKDQ31mAAAgJ28fk9O27Zt1atXL495rVu3Vvv27Z35qampmjBhgiIiIhQaGqr/+I//UEJCgq677jpJ0uDBg9WjRw/df//9mjlzplwul6ZOnaq0tDQFBQVJkh555BG9+uqrmjRpkh566CFt2rRJ77zzjt5/n3/wBgAA6unG4/N55ZVX5O/vr+HDh6uyslJJSUn661//6rS3aNFC69at06OPPqqEhAS1bt1ao0aN0vTp052auLg4vf/++xo/frzmzp2ryy67TK+//rqSkpIaY0gAAKCJaZCQ8/HHH3v8HhwcrAULFmjBggVnfUznzp21fv36cy735ptv1q5du7zRRQAAYBn+dxUA4Ly6TOZWADQ/hBwAAGAlQg4AALASIQcAAFiJkAP8P9xzAAB2IeQAAAArEXIAAPWOK6XexzY9P0IOAACwEiEHAABYiZADAACsRMgBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAA0MzwzzkvDCEHAABYiZADNAD+6gKAhkfIAQAAViLkAACaPK6G4mIQcgAAgJUIOWhQ/DUGAN7DMfXcCDkAADQQQknDIuQAAAArEXKAJuRS/8rjr0T4Mp7/+DVCDgAAsBIhBwCaEK5GAN5DyAEAAFYi5OCi8RfnmdgmANB0EHIAAICVCDloErgC0jSwHwDYhJCDZoMTMACgLgg5AADASoQcAABgJUIOAACwEiEHAABYiZADABbhBn3g/0fIgTU4uAMATkfIAS4AAQpAfbqUYwzHp7PzesjJzMzUNddco7Zt2yoyMlLDhg1TSUmJR83x48eVlpam9u3bq02bNho+fLhKS0s9ag4ePKjk5GS1atVKkZGRmjhxok6ePOlR8/HHH+vqq69WUFCQrrjiCi1btszbwwEAAM2U10PO5s2blZaWpq1btyonJ0cnTpzQ4MGDdezYMadm/Pjxeu+997Rq1Spt3rxZhw8f1l133eW0nzp1SsnJyaqqqtKWLVv05ptvatmyZcrIyHBqDhw4oOTkZN1yyy0qKirSuHHj9PDDD2vDhg3eHhIAAGiGAry9wOzsbI/fly1bpsjISBUWFuqmm25SRUWF3njjDS1fvly///3vJUlLly5V9+7dtXXrVl133XXauHGjvvjiC3344YeKiorSVVddpRkzZuiJJ57QtGnTFBgYqEWLFikuLk6zZs2SJHXv3l2ffvqpXnnlFSUlJXl7WAAAoJmp93tyKioqJEkRERGSpMLCQp04cUKJiYlOTbdu3dSpUyfl5+dLkvLz89W7d29FRUU5NUlJSXK73SouLnZqTl9GTU3NMmpTWVkpt9vtMQEAADvVa8iprq7WuHHjdMMNN6hXr16SJJfLpcDAQIWHh3vURkVFyeVyOTWnB5ya9pq2c9W43W79/PPPtfYnMzNTYWFhzhQbG3vJYwQAAE1TvYactLQ07d27VytWrKjP1VywKVOmqKKiwpkOHTrU2F0CAAD1xOv35NRIT0/XunXrlJeXp8suu8yZHx0draqqKpWXl3tczSktLVV0dLRTs23bNo/l1Xz66vSaX38iq7S0VKGhoQoJCam1T0FBQQoKCrrksQEAgKbP61dyjDFKT0/X6tWrtWnTJsXFxXm09+/fXy1btlRubq4zr6SkRAcPHlRCQoIkKSEhQXv27FFZWZlTk5OTo9DQUPXo0cOpOX0ZNTU1y0DD4PsZ0BB4ngG4GF4POWlpaXr77be1fPlytW3bVi6XSy6Xy7lPJiwsTKmpqZowYYI++ugjFRYW6sEHH1RCQoKuu+46SdLgwYPVo0cP3X///fr888+1YcMGTZ06VWlpac6VmEceeUTffPONJk2apC+//FJ//etf9c4772j8+PHeHhIAoJnzRlAmbDc/Xg85CxcuVEVFhW6++WZ17NjRmVauXOnUvPLKK7rttts0fPhw3XTTTYqOjta7777rtLdo0ULr1q1TixYtlJCQoPvuu08PPPCApk+f7tTExcXp/fffV05Ojvr27atZs2bp9ddf5+PjDYAXOmAvXt+widfvyTHGnLcmODhYCxYs0IIFC85a07lzZ61fv/6cy7n55pu1a9euOvcRAADYj/9dBQAArETIAQAAViLkAD6A+ywA+CJCDgAAsBIhB0C940oSgMZAyAFwXoQUAM0RIQcAAFiJkAMAOKe6XMnjqh+aEkIOAACwEiEHAABYiZADAACsRMiBT+A+AcA3NOZrneNM00PIAQAAViLkAIDluMKA2vjC84KQAwAArETIAQAAViLkAAAAKxFyAACAlQg5uGS+cPMaADQFHG/rhpADAKfhJALYg5ADAICP8ZUwT8gBYBUbD942jgloCIQcNBgO1ACAhkTIQZ0QVFDfeI4B8BZCDtDEcdIHcD4cJ2pHyAHgVRxsATQVhBw0GZwcAQDeRMjxUQQKAIBk9/mAkAPUM5sPILgwPAeAxkHIAQDg/yGQ2oWQg2aBAw8A23Gc8z5CDgCcBycf2MSXns+EHAAAGoAvhYumgpDjg05/ofGia3rYJwDgHYQcWKU+AgKhAwCaJ0IO4GWEIgBoGgg5AIAmhT8U4C2EHABAk0bowcUi5ACW4YQAAL8g5ICT4q+wPQBcLI4fTQshBwAAC1xKwLI1nBFyfIwvvgiaa78BXBxe86hByIHXcGCxE/u14TX2Nm/s9XubLeOxZRwNqdmHnAULFqhLly4KDg5WfHy8tm3b1thdAuDjGvtk1NjrB5qKZh1yVq5cqQkTJujpp5/Wzp071bdvXyUlJamsrKyxu4aL5I2Ds00HeBvGYsMY4Ft89Tl7MeNu6tuqWYec2bNna/To0XrwwQfVo0cPLVq0SK1atdKSJUsau2vNzvmeqE39iQxPNfvrUvdbU9jvTaEPQEOpz+d7fS27Kb9GAxq7AxerqqpKhYWFmjJlijPP399fiYmJys/Pr/UxlZWVqqysdH6vqKiQJLnd7vrtrJf1enqDJGnvM0l1fmx15U/Oz263+4zfL+RxZ3tsdeVPtS6j19MbtPeZpDOWUZd+nK/ftfXvYh5b2zhq2i/0saevv2Zfna2f51pWXdZ9rmXWpd81v59tn13osjqNX+U8P8/Xz9qWdTHb/9fLqlHbWC5kHKc//mz9PNfvZ3sOXuiyvLUNavp/Mc+D2sZfl8fWPL6ur/9zbb8Lee7/ug+X8ho+2xgu9LGnu5Rj4LnGcbrTX3u1Pb4u++5C1Dz2XOutDzX9NMacu9A0U999952RZLZs2eIxf+LEiebaa6+t9TFPP/20kcTExMTExMRkwXTo0KFzZoVmeyXnYkyZMkUTJkxwfq+urtaRI0fUvn17+fn5eW09brdbsbGxOnTokEJDQ7223KbGF8bpC2OUfGOcvjBGiXHaxBfGKF3cOI0x+vHHHxUTE3POumYbcjp06KAWLVqotLTUY35paamio6NrfUxQUJCCgoI85oWHh9dXFxUaGmr1E7OGL4zTF8Yo+cY4fWGMEuO0iS+MUar7OMPCws5b02xvPA4MDFT//v2Vm5vrzKuurlZubq4SEhIasWcAAKApaLZXciRpwoQJGjVqlAYMGKBrr71Wc+bM0bFjx/Tggw82dtcAAEAja9Yh5+6779Y///lPZWRkyOVy6aqrrlJ2draioqIatV9BQUF6+umnz3hrzDa+ME5fGKPkG+P0hTFKjNMmvjBGqX7H6WfM+T5/BQAA0Pw023tyAAAAzoWQAwAArETIAQAAViLkAAAAKxFy6sGCBQvUpUsXBQcHKz4+Xtu2bWvsLl2wvLw83X777YqJiZGfn5/WrFnj0W6MUUZGhjp27KiQkBAlJiZq//79HjVHjhxRSkqKQkNDFR4ertTUVB09erQBR3FumZmZuuaaa9S2bVtFRkZq2LBhKikp8ag5fvy40tLS1L59e7Vp00bDhw8/44snDx48qOTkZLVq1UqRkZGaOHGiTp482ZBDOaeFCxeqT58+zhdsJSQk6IMPPnDabRjjr73wwgvy8/PTuHHjnHk2jHPatGny8/PzmLp16+a02zBGSfruu+903333qX379goJCVHv3r21Y8cOp92G40+XLl3O2Jd+fn5KS0uTZMe+PHXqlJ566inFxcUpJCREl19+uWbMmOHxf6YabF9e+n+RwulWrFhhAgMDzZIlS0xxcbEZPXq0CQ8PN6WlpY3dtQuyfv1685e//MW8++67RpJZvXq1R/sLL7xgwsLCzJo1a8znn39u/vCHP5i4uDjz888/OzVDhgwxffv2NVu3bjWffPKJueKKK8w999zTwCM5u6SkJLN06VKzd+9eU1RUZG699VbTqVMnc/ToUafmkUceMbGxsSY3N9fs2LHDXHfddeb666932k+ePGl69eplEhMTza5du8z69etNhw4dzJQpUxpjSLVau3atef/9981XX31lSkpKzJNPPmlatmxp9u7da4yxY4yn27Ztm+nSpYvp06ePGTt2rDPfhnE+/fTTpmfPnub77793pn/+859Ouw1jPHLkiOncubP505/+ZAoKCsw333xjNmzYYL7++munxobjT1lZmcd+zMnJMZLMRx99ZIyxY18+99xzpn379mbdunXmwIEDZtWqVaZNmzZm7ty5Tk1D7UtCjpdde+21Ji0tzfn91KlTJiYmxmRmZjZiry7Or0NOdXW1iY6ONi+99JIzr7y83AQFBZm//e1vxhhjvvjiCyPJbN++3an54IMPjJ+fn/nuu+8arO91UVZWZiSZzZs3G2N+GVPLli3NqlWrnJp9+/YZSSY/P98Y80sY9Pf3Ny6Xy6lZuHChCQ0NNZWVlQ07gDpo166def31160b448//miuvPJKk5OTY/71X//VCTm2jPPpp582ffv2rbXNljE+8cQTZuDAgWdtt/X4M3bsWHP55Zeb6upqa/ZlcnKyeeihhzzm3XXXXSYlJcUY07D7krervKiqqkqFhYVKTEx05vn7+ysxMVH5+fmN2DPvOHDggFwul8f4wsLCFB8f74wvPz9f4eHhGjBggFOTmJgof39/FRQUNHifL0RFRYUkKSIiQpJUWFioEydOeIyzW7du6tSpk8c4e/fu7fHFk0lJSXK73SouLm7A3l+YU6dOacWKFTp27JgSEhKsG2NaWpqSk5M9xiPZtS/379+vmJgY/e53v1NKSooOHjwoyZ4xrl27VgMGDNCIESMUGRmpfv366bXXXnPabTz+VFVV6e2339ZDDz0kPz8/a/bl9ddfr9zcXH311VeSpM8//1yffvqphg4dKqlh92Wz/sbjpuZ///d/derUqTO+cTkqKkpffvllI/XKe1wulyTVOr6aNpfLpcjISI/2gIAARUREODVNSXV1tcaNG6cbbrhBvXr1kvTLGAIDA8/4562/Hmdt26GmranYs2ePEhISdPz4cbVp00arV69Wjx49VFRUZM0YV6xYoZ07d2r79u1ntNmyL+Pj47Vs2TJ17dpV33//vZ555hndeOON2rt3rzVj/Oabb7Rw4UJNmDBBTz75pLZv367HHntMgYGBGjVqlJXHnzVr1qi8vFx/+tOfJNnzfJ08ebLcbre6deumFi1a6NSpU3ruueeUkpIiqWHPJYQc+LS0tDTt3btXn376aWN3pV507dpVRUVFqqio0N///neNGjVKmzdvbuxuec2hQ4c0duxY5eTkKDg4uLG7U29q/gKWpD59+ig+Pl6dO3fWO++8o5CQkEbsmfdUV1drwIABev755yVJ/fr10969e7Vo0SKNGjWqkXtXP9544w0NHTpUMTExjd0Vr3rnnXeUlZWl5cuXq2fPnioqKtK4ceMUExPT4PuSt6u8qEOHDmrRosUZd8KXlpYqOjq6kXrlPTVjONf4oqOjVVZW5tF+8uRJHTlypMltg/T0dK1bt04fffSRLrvsMmd+dHS0qqqqVF5e7lH/63HWth1q2pqKwMBAXXHFFerfv78yMzPVt29fzZ0715oxFhYWqqysTFdffbUCAgIUEBCgzZs3a968eQoICFBUVJQV4/y18PBw/cu//Iu+/vpra/Zlx44d1aNHD4953bt3d96Ws+348+233+rDDz/Uww8/7MyzZV9OnDhRkydP1siRI9W7d2/df//9Gj9+vDIzMyU17L4k5HhRYGCg+vfvr9zcXGdedXW1cnNzlZCQ0Ig98464uDhFR0d7jM/tdqugoMAZX0JCgsrLy1VYWOjUbNq0SdXV1YqPj2/wPtfGGKP09HStXr1amzZtUlxcnEd7//791bJlS49xlpSU6ODBgx7j3LNnj8eLMCcnR6GhoWccqJuS6upqVVZWWjPGQYMGac+ePSoqKnKmAQMGKCUlxfnZhnH+2tGjR/Xf//3f6tixozX78oYbbjjjqxy++uorde7cWZI9x58aS5cuVWRkpJKTk515tuzLn376Sf7+nvGiRYsWqq6ultTA+/ISbqBGLVasWGGCgoLMsmXLzBdffGHGjBljwsPDPe6Eb8p+/PFHs2vXLrNr1y4jycyePdvs2rXLfPvtt8aYXz72Fx4ebv7rv/7L7N6929xxxx21fuyvX79+pqCgwHz66afmyiuvbFIf4Xz00UdNWFiY+fjjjz0+yvnTTz85NY888ojp1KmT2bRpk9mxY4dJSEgwCQkJTnvNxzgHDx5sioqKTHZ2tvnNb37TpD7GOXnyZLN582Zz4MABs3v3bjN58mTj5+dnNm7caIyxY4y1Of3TVcbYMc7HH3/cfPzxx+bAgQPms88+M4mJiaZDhw6mrKzMGGPHGLdt22YCAgLMc889Z/bv32+ysrJMq1atzNtvv+3U2HD8MeaXT9126tTJPPHEE2e02bAvR40aZX772986HyF/9913TYcOHcykSZOcmobal4ScejB//nzTqVMnExgYaK699lqzdevWxu7SBfvoo4+MpDOmUaNGGWN++ejfU089ZaKiokxQUJAZNGiQKSkp8VjG//3f/5l77rnHtGnTxoSGhpoHH3zQ/Pjjj40wmtrVNj5JZunSpU7Nzz//bP785z+bdu3amVatWpk777zTfP/99x7L+cc//mGGDh1qQkJCTIcOHczjjz9uTpw40cCjObuHHnrIdO7c2QQGBprf/OY3ZtCgQU7AMcaOMdbm1yHHhnHefffdpmPHjiYwMND89re/NXfffbfH98fYMEZjjHnvvfdMr169TFBQkOnWrZtZvHixR7sNxx9jjNmwYYORdEbfjbFjX7rdbjN27FjTqVMnExwcbH73u9+Zv/zlLx4fcW+ofelnzGlfQQgAAGAJ7skBAABWIuQAAAArEXIAAICVCDkAAMBKhBwAAGAlQg4AALASIQcAAFiJkAMAAKxEyAEAAFYi5AAAACsRcgAAgJUIOQAAwEr/H07fP8TQgfuPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Había quedado pendiente: un plot de la varianza de cada dimensión\n",
    "plt.bar(np.arange(len(X_sample.columns)),X_sample.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ea917",
   "metadata": {},
   "source": [
    "Un aspecto bien importante, es que las redes autoencoders suelen trabajar mejor cuando podemos estandarizar los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a509c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### También vamos a estandarizar los datos en X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_std = sc.fit_transform(X_sample)\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5afab2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.00005000e-02, -1.00005000e-02, -1.00005000e-02,\n",
       "       -1.39737990e-02, -1.89315247e-02, -2.31843010e-02, -3.60728861e-02,\n",
       "       -3.92619154e-02, -3.80269994e-02, -3.90143887e-02, -3.46046778e-02,\n",
       "       -2.57765396e-02, -2.09733754e-02, -2.17809993e-02, -1.44984527e-02,\n",
       "       -1.18807892e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.78081425e-02, -2.32058779e-02, -2.98662898e-02,\n",
       "       -4.14395151e-02, -5.86512813e-02, -8.12643979e-02, -1.05997038e-01,\n",
       "       -1.21704878e-01, -1.34457288e-01, -1.39756261e-01, -1.41562422e-01,\n",
       "       -1.35229133e-01, -1.20246727e-01, -1.04490087e-01, -8.70044931e-02,\n",
       "       -7.16699334e-02, -4.85892545e-02, -3.24260775e-02, -2.16926329e-02,\n",
       "       -1.00005000e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.32956624e-02, -2.25936238e-02, -3.83702224e-02, -5.98206019e-02,\n",
       "       -8.42014426e-02, -1.18390816e-01, -1.54266827e-01, -1.88282524e-01,\n",
       "       -2.19803054e-01, -2.42936317e-01, -2.55020324e-01, -2.59481423e-01,\n",
       "       -2.49404582e-01, -2.26727106e-01, -2.00418885e-01, -1.67161170e-01,\n",
       "       -1.34317009e-01, -9.58717755e-02, -7.36565245e-02, -5.03983075e-02,\n",
       "       -2.69783475e-02, -1.68919000e-02, -1.00005000e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.47795885e-02,\n",
       "       -2.51221010e-02, -3.81226487e-02, -7.86317321e-02, -1.19593671e-01,\n",
       "       -1.65704529e-01, -2.28814281e-01, -2.88620224e-01, -3.54491034e-01,\n",
       "       -4.21140618e-01, -4.80243669e-01, -5.27064646e-01, -5.40807419e-01,\n",
       "       -5.21388017e-01, -4.74446021e-01, -4.03948632e-01, -3.36571539e-01,\n",
       "       -2.71580657e-01, -2.06667410e-01, -1.54539645e-01, -1.08856709e-01,\n",
       "       -6.77589146e-02, -3.40327281e-02, -2.15091205e-02,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.00005000e-02, -1.07381289e-02, -2.60253876e-02,\n",
       "       -5.70600482e-02, -9.14378767e-02, -1.43000013e-01, -1.99005834e-01,\n",
       "       -2.66034404e-01, -3.53401549e-01, -4.50251488e-01, -5.51598332e-01,\n",
       "       -6.47939202e-01, -7.43171364e-01, -8.18162561e-01, -8.51073275e-01,\n",
       "       -8.31121680e-01, -7.63764496e-01, -6.59992784e-01, -5.47527626e-01,\n",
       "       -4.39376979e-01, -3.35576590e-01, -2.54856553e-01, -1.83933732e-01,\n",
       "       -1.26755715e-01, -7.06477667e-02, -3.88818206e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.34176155e-02, -3.90612132e-02,\n",
       "       -8.73974922e-02, -1.33107017e-01, -1.94532142e-01, -2.74786330e-01,\n",
       "       -3.69886454e-01, -4.82920333e-01, -6.05294063e-01, -7.35621386e-01,\n",
       "       -8.69509827e-01, -9.89564738e-01, -1.09132506e+00, -1.13182948e+00,\n",
       "       -1.09408349e+00, -9.96373436e-01, -8.68781173e-01, -7.17778845e-01,\n",
       "       -5.70649327e-01, -4.39021868e-01, -3.26889344e-01, -2.35934504e-01,\n",
       "       -1.67697996e-01, -9.95100269e-02, -4.79392976e-02, -1.87851186e-02,\n",
       "        0.00000000e+00, -1.17322667e-02, -2.88274493e-02, -6.46532861e-02,\n",
       "       -1.18956716e-01, -1.77837580e-01,  1.53795878e+00,  2.57176245e+00,\n",
       "        1.53212043e+00,  1.00392168e+00, -1.79355647e-01, -5.91732991e-01,\n",
       "       -1.05273662e+00, -1.15378689e+00, -1.22142979e+00, -1.23881560e+00,\n",
       "       -1.21321586e+00, -1.14302847e+00, -1.02018313e+00, -8.57098743e-01,\n",
       "       -6.76706697e-01, -5.16203262e-01, -3.79287244e-01, -2.71402545e-01,\n",
       "       -1.89934521e-01, -1.19940614e-01, -5.56340911e-02, -1.45752163e-02,\n",
       "        0.00000000e+00, -2.06611389e-02, -4.37166621e-02, -8.08756237e-02,\n",
       "       -1.40488164e-01, -2.07699245e-01,  3.77477260e+00,  3.14033146e+00,\n",
       "        2.28939169e+00,  1.76127332e+00,  1.43185420e+00,  1.13131350e+00,\n",
       "        6.79164893e-01,  6.65484747e-01,  6.66043389e-01,  6.80680095e-01,\n",
       "        6.77305174e-01,  6.65508286e-01,  7.21340316e-01,  8.83661589e-01,\n",
       "        9.17518690e-01,  2.82541074e-02, -4.01002939e-01, -2.83099723e-01,\n",
       "       -1.94831338e-01, -1.23075256e-01, -6.66126860e-02, -1.61462821e-02,\n",
       "       -1.12546885e-02, -2.93918605e-02, -4.84646663e-02, -9.31783260e-02,\n",
       "       -1.46682925e-01, -2.18121209e-01,  8.30460131e-01,  1.04725853e+00,\n",
       "        1.47086928e-01,  2.59684517e-01,  4.95679969e-01,  9.98953721e-01,\n",
       "        1.29535061e+00,  1.12204782e+00,  1.41528197e+00,  1.42599520e+00,\n",
       "        1.36416372e+00,  1.22805443e+00,  1.03395727e+00,  1.40874227e+00,\n",
       "        1.73166837e+00,  1.00260058e+00, -4.01823716e-01, -2.75049233e-01,\n",
       "       -1.81713744e-01, -1.07567122e-01, -5.66041118e-02, -1.89159236e-02,\n",
       "       -1.21427928e-02, -2.43168731e-02, -5.02703770e-02, -8.87358114e-02,\n",
       "       -1.38806025e-01, -2.12706019e-01, -3.21729999e-01, -4.62313723e-01,\n",
       "       -6.52442841e-01, -8.45524923e-01, -9.61258323e-01, -7.93125052e-01,\n",
       "       -2.26359955e-01, -6.40468216e-01, -1.23720090e-01, -1.67157468e-01,\n",
       "       -2.55843161e-01, -4.41448335e-01, -7.92766628e-01,  1.30597044e+00,\n",
       "        1.81460411e+00,  6.91054579e-01, -3.83665051e-01, -2.63105130e-01,\n",
       "       -1.66473946e-01, -7.99663431e-02, -4.55007946e-02, -1.95541446e-02,\n",
       "       -1.00005000e-02, -1.86206584e-02, -4.14986832e-02, -7.22615997e-02,\n",
       "       -1.23238725e-01, -2.12256343e-01, -3.31309824e-01, -4.91126078e-01,\n",
       "       -6.87704902e-01, -8.62602670e-01, -9.39124713e-01, -8.69991467e-01,\n",
       "       -7.58168797e-01, -7.22198511e-01, -7.39826964e-01, -8.09980626e-01,\n",
       "       -9.11188613e-01, -1.00032001e+00, -2.21550751e-01,  1.53134484e+00,\n",
       "        1.47605194e+00, -2.73150738e-01, -3.63157263e-01, -2.52975575e-01,\n",
       "       -1.57152039e-01, -6.52009258e-02, -3.35283586e-02, -1.24209728e-02,\n",
       "        0.00000000e+00, -1.48492790e-02, -3.29699917e-02, -6.01451792e-02,\n",
       "       -1.18353377e-01, -2.19271688e-01, -3.54392407e-01, -5.23006773e-01,\n",
       "       -7.15682870e-01, -8.62626101e-01, -9.05242890e-01, -8.31592288e-01,\n",
       "       -7.51312636e-01, -7.62948163e-01, -8.25877849e-01, -9.30232292e-01,\n",
       "       -1.04727288e+00, -8.79016953e-01,  1.11455708e+00,  1.61660969e+00,\n",
       "        2.64000765e-01, -4.64282235e-01, -3.54907482e-01, -2.56014147e-01,\n",
       "       -1.58427696e-01, -6.20647188e-02, -2.42921899e-02,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.17874599e-02, -2.52632841e-02, -5.02423656e-02,\n",
       "       -1.15068847e-01, -2.35195531e-01, -3.77531303e-01, -5.47311188e-01,\n",
       "       -7.23069536e-01, -8.48981953e-01, -8.78897369e-01, -8.26469482e-01,\n",
       "       -7.95496372e-01, -8.83536617e-01, -9.94814123e-01, -1.13364619e+00,\n",
       "       -1.20871511e+00,  5.60198157e-05,  1.28700658e+00,  1.50082995e+00,\n",
       "       -1.22561277e-01, -4.62110102e-01, -3.60151562e-01, -2.63898374e-01,\n",
       "       -1.66295096e-01, -5.68635009e-02, -1.05441394e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.66367790e-02, -4.23254862e-02,\n",
       "       -1.19931644e-01, -2.52550583e-01, -3.91916340e-01, -5.56171069e-01,\n",
       "       -7.17849905e-01, -8.29516019e-01, -8.54549188e-01, -8.45989670e-01,\n",
       "       -8.89246054e-01, -1.03761315e+00, -1.16457617e+00, -1.30025654e+00,\n",
       "       -7.40699086e-01,  1.05188993e+00,  1.30369880e+00, -1.63440609e-01,\n",
       "       -5.90584640e-01, -4.74233049e-01, -3.68789557e-01, -2.74082099e-01,\n",
       "       -1.74264813e-01, -6.96188843e-02, -1.80031510e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.68610568e-02, -4.51688568e-02,\n",
       "       -1.31668459e-01, -2.67838929e-01, -3.98906806e-01, -5.48202377e-01,\n",
       "       -6.90077015e-01, -7.89823563e-01, -8.31599129e-01, -8.61314493e-01,\n",
       "       -9.56815660e-01, -1.11036634e+00, -1.22743073e+00, -1.31006468e+00,\n",
       "       -2.57368600e-02,  1.14239899e+00,  7.61423491e-01, -7.06825874e-01,\n",
       "       -6.08999426e-01, -4.92457882e-01, -3.80502867e-01, -2.79282191e-01,\n",
       "       -1.73984018e-01, -7.67235054e-02, -1.95871373e-02, -1.00005000e-02,\n",
       "        0.00000000e+00, -1.00005000e-02, -2.48178080e-02, -5.52275065e-02,\n",
       "       -1.48243512e-01, -2.83202341e-01, -4.02212500e-01, -5.34598048e-01,\n",
       "       -6.56007943e-01, -7.38083794e-01, -7.81657503e-01, -8.24620535e-01,\n",
       "       -9.18824463e-01, -1.04078449e+00, -1.13391454e+00, -1.09212795e+00,\n",
       "        7.05920310e-01,  1.17679031e+00, -3.73781820e-01, -7.58547572e-01,\n",
       "       -6.28680640e-01, -5.01492113e-01, -3.81043892e-01, -2.70505206e-01,\n",
       "       -1.68251255e-01, -7.84168728e-02, -2.27999680e-02, -1.57856413e-02,\n",
       "        0.00000000e+00,  0.00000000e+00, -2.69850288e-02, -6.76999793e-02,\n",
       "       -1.67498207e-01, -2.98089736e-01, -4.11096027e-01, -5.22810883e-01,\n",
       "       -6.25838621e-01, -6.93423683e-01, -7.31704263e-01, -7.67086709e-01,\n",
       "       -8.29980030e-01, -9.21590434e-01, -1.00562716e+00,  7.79492952e-02,\n",
       "        1.22959017e+00,  6.36500653e-01, -9.01400043e-01, -7.69630793e-01,\n",
       "       -6.35363773e-01, -4.94618472e-01, -3.69117095e-01, -2.55794246e-01,\n",
       "       -1.56732083e-01, -7.83809414e-02, -2.67109338e-02, -1.48726634e-02,\n",
       "        0.00000000e+00, -1.00005000e-02, -3.48385687e-02, -8.69311199e-02,\n",
       "       -1.85622432e-01, -3.11777198e-01, -4.27690033e-01, -5.30457702e-01,\n",
       "       -6.12837575e-01, -6.69073252e-01, -7.06628103e-01, -7.37178903e-01,\n",
       "       -7.79583917e-01, -8.66698428e-01, -2.88157768e-01,  1.21930590e+00,\n",
       "        1.10500698e+00, -5.04139890e-01, -9.09137779e-01, -7.74520432e-01,\n",
       "       -6.19405771e-01, -4.72096102e-01, -3.44822207e-01, -2.35626373e-01,\n",
       "       -1.44455008e-01, -7.69092863e-02, -2.86146987e-02, -1.00005000e-02,\n",
       "        0.00000000e+00, -1.00005000e-02, -3.42628198e-02, -1.01174053e-01,\n",
       "       -1.95711272e-01, -3.24606261e-01, -4.42716711e-01, -5.45960978e-01,\n",
       "       -6.37281741e-01, -7.03742928e-01, -7.53441795e-01, -7.88772419e-01,\n",
       "       -8.29773267e-01, -7.45526297e-01,  9.49893727e-01,  1.18293215e+00,\n",
       "        3.85795002e-01, -1.02329900e+00, -8.98728840e-01, -7.36858006e-01,\n",
       "       -5.75258663e-01, -4.30322485e-01, -3.09120250e-01, -2.09889823e-01,\n",
       "       -1.31895170e-01, -7.31506415e-02, -2.76674735e-02, -1.00005000e-02,\n",
       "        0.00000000e+00, -1.00005000e-02, -4.00234981e-02, -1.07093740e-01,\n",
       "       -1.94645695e-01, -3.16981297e-01, -4.40895564e-01, -5.60086039e-01,\n",
       "       -6.67605659e-01, -7.63806998e-01, -8.43535003e-01, -9.03604039e-01,\n",
       "       -9.38010529e-01,  7.63887624e-01,  1.12176928e+00,  7.84111000e-01,\n",
       "       -8.18046093e-01, -9.91046672e-01, -8.28340182e-01, -6.52780006e-01,\n",
       "       -4.95325185e-01, -3.64891317e-01, -2.61772085e-01, -1.75298870e-01,\n",
       "       -1.12966586e-01, -6.17374486e-02, -2.70715466e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -4.06825662e-02, -9.78606438e-02,\n",
       "       -1.77848987e-01, -2.87783481e-01, -4.12614752e-01, -5.43271605e-01,\n",
       "       -6.71018812e-01, -7.98159188e-01, -9.16686263e-01, -1.02499517e+00,\n",
       "       -7.73682132e-01,  1.09355574e+00,  1.05041156e+00, -4.98209852e-01,\n",
       "       -1.05256459e+00, -8.70980804e-01, -6.88431167e-01, -5.23166414e-01,\n",
       "       -3.91308572e-01, -2.82035183e-01, -1.99071147e-01, -1.36525170e-01,\n",
       "       -8.93688913e-02, -4.13170860e-02, -1.68508310e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -2.83386899e-02, -7.65120563e-02,\n",
       "       -1.41969555e-01, -2.32658498e-01, -3.41261378e-01, -4.69723228e-01,\n",
       "       -6.06194512e-01, -7.47366354e-01, -8.80786554e-01, -7.29389144e-01,\n",
       "        8.95224865e-01,  1.11943124e+00, -1.05438374e-01, -1.00783177e+00,\n",
       "       -8.59696548e-01, -6.83890026e-01, -5.31181637e-01, -3.95889778e-01,\n",
       "       -2.89956123e-01, -2.03267966e-01, -1.42951450e-01, -9.63532989e-02,\n",
       "       -6.43914026e-02, -3.37070214e-02, -1.11853003e-02,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.00005000e-02, -1.51722732e-02, -4.80051146e-02,\n",
       "       -9.51161616e-02, -1.60643556e-01, -2.45453283e-01, -3.53245922e-01,\n",
       "       -4.74265429e-01, -5.98667391e-01, -7.29305101e-01,  3.89322873e-01,\n",
       "        1.38694264e+00,  1.37486731e+00, -4.03963644e-01, -7.74445930e-01,\n",
       "       -6.38730244e-01, -5.02999283e-01, -3.87339921e-01, -2.79971294e-01,\n",
       "       -1.98381814e-01, -1.35822721e-01, -9.65383286e-02, -6.33365644e-02,\n",
       "       -4.27549534e-02, -2.57581657e-02, -1.00005000e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -2.37543896e-02,\n",
       "       -5.22032466e-02, -8.58749627e-02, -1.40703979e-01, -2.08515621e-01,\n",
       "       -2.90149335e-01, -3.68567087e-01,  3.34201602e-01,  2.33307288e+00,\n",
       "        2.27286258e+00,  2.23777229e+00,  4.12218057e-02, -4.94890333e-01,\n",
       "       -4.22342015e-01, -3.39048837e-01, -2.57069088e-01, -1.85534152e-01,\n",
       "       -1.36577185e-01, -8.60242391e-02, -5.78259874e-02, -3.36364160e-02,\n",
       "       -1.81122384e-02, -1.00005000e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.36274661e-02,\n",
       "       -2.85803164e-02, -4.74793553e-02, -7.79785591e-02, -1.18532172e-01,\n",
       "       -1.67201555e-01, -2.14787719e-01,  2.22171299e+00,  4.30500754e+00,\n",
       "        4.03125111e+00,  3.36505818e+00,  3.79953648e-01, -2.84269948e-01,\n",
       "       -2.47694588e-01, -2.05869945e-01, -1.55925102e-01, -1.16435448e-01,\n",
       "       -8.57647974e-02, -5.46508166e-02, -4.01800073e-02, -2.37589970e-02,\n",
       "       -1.65780693e-02, -1.00005000e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.15748833e-02, -2.84271584e-02, -5.06655656e-02, -7.40332846e-02,\n",
       "       -1.00455604e-01, -1.24744578e-01,  4.17363552e+00,  7.81243004e+00,\n",
       "        5.78969790e+00,  3.22149281e-01, -1.81506609e-01, -1.60333393e-01,\n",
       "       -1.39182079e-01, -1.18875455e-01, -8.73316648e-02, -7.00227708e-02,\n",
       "       -5.40690537e-02, -3.84297037e-02, -2.65616274e-02, -1.61844507e-02,\n",
       "       -1.19683967e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.32918601e-02, -1.59980455e-02,\n",
       "       -2.07236291e-02, -2.66997366e-02, -2.84703819e-02, -3.43035092e-02,\n",
       "       -4.10336906e-02, -4.88886427e-02, -5.48357917e-02, -5.51988782e-02,\n",
       "       -4.69971082e-02, -3.88769026e-02, -3.16010302e-02, -2.85226846e-02,\n",
       "       -2.17365890e-02, -1.00005000e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76766ff1",
   "metadata": {},
   "source": [
    "## Reduciendo a 5 dimensiones, autoencoder\n",
    "\n",
    "Vamos a ver ahora como usamos un autoencoder para reducir nuestra dimensionalidad. Nuestro primer modelo es el más simple: una primera capa recibe un input de tamaño *num_dimensiones* y lo lleva a 5 neuronas. Luego esas 5 neuronas son \"decodificadas\" en un dataset con *num_dimensiones*.\n",
    "\n",
    "Como esto es un autoencoder, vamos a separar conceptualmente las capas en dos. \n",
    "- una capa encoder que reduce la dimensionalidad\n",
    "- una capa decoder que toma la dimensionalidad reducida y la devuelve a todas las dimensiones. \n",
    "\n",
    "La gracia es que podemos entrenar el autoencoder comparando los datos con los datos que fueron codificados y luego decodificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afef355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 17:49:39.722377: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-15 17:49:41.918738: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-15 17:49:43.363718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-15 17:49:43.653004: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-15 17:49:43.710913: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 17:49:44.590945: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 17:49:51.370406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 17:49:53.294362: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 21952000 exceeds 10% of free system memory.\n",
      "2024-10-15 17:49:53.302318: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 21952000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8599 - val_loss: 5.4028\n",
      "Epoch 2/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8713 - val_loss: 5.3780\n",
      "Epoch 3/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8372 - val_loss: 5.3448\n",
      "Epoch 4/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8427 - val_loss: 5.3194\n",
      "Epoch 5/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7587 - val_loss: 5.2976\n",
      "Epoch 6/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7232 - val_loss: 5.2804\n",
      "Epoch 7/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7001 - val_loss: 5.2690\n",
      "Epoch 8/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6741 - val_loss: 5.2612\n",
      "Epoch 9/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6841 - val_loss: 5.2560\n",
      "Epoch 10/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6813 - val_loss: 5.2522\n",
      "Epoch 11/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6841 - val_loss: 5.2492\n",
      "Epoch 12/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6797 - val_loss: 5.2462\n",
      "Epoch 13/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6351 - val_loss: 5.2442\n",
      "Epoch 14/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6891 - val_loss: 5.2418\n",
      "Epoch 15/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6824 - val_loss: 5.2397\n",
      "Epoch 16/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6862 - val_loss: 5.2381\n",
      "Epoch 17/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6745 - val_loss: 5.2365\n",
      "Epoch 18/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6974 - val_loss: 5.2353\n",
      "Epoch 19/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6896 - val_loss: 5.2336\n",
      "Epoch 20/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6071 - val_loss: 5.2326\n",
      "Epoch 21/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6701 - val_loss: 5.2317\n",
      "Epoch 22/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6643 - val_loss: 5.2307\n",
      "Epoch 23/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6424 - val_loss: 5.2295\n",
      "Epoch 24/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6728 - val_loss: 5.2286\n",
      "Epoch 25/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6749 - val_loss: 5.2282\n",
      "Epoch 26/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6960 - val_loss: 5.2270\n",
      "Epoch 27/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6588 - val_loss: 5.2262\n",
      "Epoch 28/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6777 - val_loss: 5.2259\n",
      "Epoch 29/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6735 - val_loss: 5.2253\n",
      "Epoch 30/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7028 - val_loss: 5.2252\n",
      "Epoch 31/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6991 - val_loss: 5.2241\n",
      "Epoch 32/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6736 - val_loss: 5.2235\n",
      "Epoch 33/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6576 - val_loss: 5.2232\n",
      "Epoch 34/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6640 - val_loss: 5.2227\n",
      "Epoch 35/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6665 - val_loss: 5.2215\n",
      "Epoch 36/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6653 - val_loss: 5.2212\n",
      "Epoch 37/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6902 - val_loss: 5.2207\n",
      "Epoch 38/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6880 - val_loss: 5.2203\n",
      "Epoch 39/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6618 - val_loss: 5.2201\n",
      "Epoch 40/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7034 - val_loss: 5.2200\n",
      "Epoch 41/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6837 - val_loss: 5.2194\n",
      "Epoch 42/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6466 - val_loss: 5.2188\n",
      "Epoch 43/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6553 - val_loss: 5.2187\n",
      "Epoch 44/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6618 - val_loss: 5.2184\n",
      "Epoch 45/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6676 - val_loss: 5.2178\n",
      "Epoch 46/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6664 - val_loss: 5.2177\n",
      "Epoch 47/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6981 - val_loss: 5.2170\n",
      "Epoch 48/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6406 - val_loss: 5.2170\n",
      "Epoch 49/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6592 - val_loss: 5.2164\n",
      "Epoch 50/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6359 - val_loss: 5.2166\n",
      "Epoch 51/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6686 - val_loss: 5.2166\n",
      "Epoch 52/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6407 - val_loss: 5.2157\n",
      "Epoch 53/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6661 - val_loss: 5.2152\n",
      "Epoch 54/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6651 - val_loss: 5.2154\n",
      "Epoch 55/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6866 - val_loss: 5.2147\n",
      "Epoch 56/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 0.6541 - val_loss: 5.2149\n",
      "Epoch 57/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6775 - val_loss: 5.2146\n",
      "Epoch 58/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7073 - val_loss: 5.2143\n",
      "Epoch 59/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6414 - val_loss: 5.2147\n",
      "Epoch 60/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6931 - val_loss: 5.2143\n",
      "Epoch 61/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6672 - val_loss: 5.2146\n",
      "Epoch 62/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6642 - val_loss: 5.2138\n",
      "Epoch 63/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6720 - val_loss: 5.2137\n",
      "Epoch 64/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 0.6831 - val_loss: 5.2137\n",
      "Epoch 65/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6689 - val_loss: 5.2136\n",
      "Epoch 66/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 0.6771 - val_loss: 5.2128\n",
      "Epoch 67/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 0.6699 - val_loss: 5.2126\n",
      "Epoch 68/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6891 - val_loss: 5.2127\n",
      "Epoch 69/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6236 - val_loss: 5.2123\n",
      "Epoch 70/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 0.6688 - val_loss: 5.2125\n",
      "Epoch 71/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6563 - val_loss: 5.2122\n",
      "Epoch 72/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6571 - val_loss: 5.2119\n",
      "Epoch 73/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6641 - val_loss: 5.2119\n",
      "Epoch 74/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6477 - val_loss: 5.2121\n",
      "Epoch 75/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6734 - val_loss: 5.2115\n",
      "Epoch 76/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6703 - val_loss: 5.2113\n",
      "Epoch 77/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 0.6724 - val_loss: 5.2113\n",
      "Epoch 78/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.6711 - val_loss: 5.2111\n",
      "Epoch 79/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 0.6657 - val_loss: 5.2110\n",
      "Epoch 80/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6789 - val_loss: 5.2115\n",
      "Epoch 81/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6765 - val_loss: 5.2115\n",
      "Epoch 82/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 0.6412 - val_loss: 5.2108\n",
      "Epoch 83/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6851 - val_loss: 5.2107\n",
      "Epoch 84/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6491 - val_loss: 5.2108\n",
      "Epoch 85/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - loss: 0.6761 - val_loss: 5.2104\n",
      "Epoch 86/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6622 - val_loss: 5.2103\n",
      "Epoch 87/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6363 - val_loss: 5.2106\n",
      "Epoch 88/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6790 - val_loss: 5.2111\n",
      "Epoch 89/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 0.6489 - val_loss: 5.2106\n",
      "Epoch 90/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6662 - val_loss: 5.2105\n",
      "Epoch 91/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6464 - val_loss: 5.2101\n",
      "Epoch 92/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6259 - val_loss: 5.2101\n",
      "Epoch 93/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6542 - val_loss: 5.2101\n",
      "Epoch 94/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 0.6750 - val_loss: 5.2105\n",
      "Epoch 95/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6543 - val_loss: 5.2100\n",
      "Epoch 96/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6770 - val_loss: 5.2100\n",
      "Epoch 97/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6328 - val_loss: 5.2101\n",
      "Epoch 98/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - loss: 0.6768 - val_loss: 5.2101\n",
      "Epoch 99/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6850 - val_loss: 5.2099\n",
      "Epoch 100/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6592 - val_loss: 5.2098\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "num_dimensiones = len(X_sample.columns)\n",
    "\n",
    "#### Sequential es un modelo en keras que simplemente va concatenando las capas, \n",
    "#### las neuronas del final de una capa son las del principio de otra\n",
    "\n",
    "#### keras.layers.Dense es una capa donde todas las neuronas están conectadas las unas a las otras\n",
    "\n",
    "#### armamos el encoder: el input conectado a 5 neuronas\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, input_shape=[num_dimensiones]),\n",
    "])\n",
    "\n",
    "#### armamos el dencoder: esas 5 neuronas luego se conectan a todas las dimensiones del input \n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(num_dimensiones, input_shape=[5]),\n",
    "])\n",
    "\n",
    "#### y ahora concatenamos encoder y decoder\n",
    "autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "#### compile lo deja listo para aprender. \n",
    "#### Para comparar el resultado del autoencoder y el dataset original \n",
    "#### usamos el clásico Mean Squared Error (a eso se le llama \"loss\")\n",
    "#### Para entrenar usamos StochasticGradientDescent, con un alfa de 0.1 \n",
    "\n",
    "autoencoder.compile(loss='mse', optimizer = keras.optimizers.SGD(learning_rate=0.1))\n",
    "\n",
    "#### El entrenamiento toma los datos originales, los codifica/decodifica en vectores de la misma \n",
    "#### dimensión y los compara con los datos originales. \n",
    "#### Esto por 100 épocas, y para validar usamos el set de test. \n",
    "#### Para no perder tiempo, decidimos que debe parar si la métrica del MSE no mejora en 10 épocas. \n",
    "\n",
    "history = autoencoder.fit(X_train_std,X_train_std, epochs=100,validation_data=(X_test_std,X_test_std),\n",
    "                         callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "                         \n",
    "#### Finalmente, para codificar solo llamamos al método predict de la parte de encoder. \n",
    "X_auto = encoder.predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cdb4831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.380753  , -0.20930114, -5.2374806 ,  2.0497098 , -2.401461  ],\n",
       "       [ 3.194425  ,  8.601458  ,  9.806052  , -4.7125626 , -1.3350602 ],\n",
       "       [-5.092676  ,  4.7467904 , -2.2331994 , -5.113067  , -0.41818038],\n",
       "       ...,\n",
       "       [ 1.668603  , -0.3462938 , -5.9983945 ,  0.27854764,  0.908656  ],\n",
       "       [-3.998729  , -3.1829572 , -2.6165366 , -5.1164813 ,  1.4926149 ],\n",
       "       [10.630365  , -6.9404297 ,  3.736921  , -5.6659603 , -3.6126857 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee833bc0",
   "metadata": {},
   "source": [
    "## Probando cuál enfoque de reducción es mejor de forma indirecta: viendo qué dataset sirve más para clasificación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada3c42",
   "metadata": {},
   "source": [
    "Pero como saber si esta codificación sirve de algo? \n",
    "Una posibilidad es probarlo de forma indirecta: ejecutando algún tipo de tarea de aprendizaje sobre estos datos, y comparando los resultados. \n",
    "Aquí, comparamos con la tarea de clasificar los números de MNIST usando RandomForest. Vamos a comparar el resultado de usar RandomForest en el todo, usando PCA, y usando estos modelos de autoencoders. \n",
    "\n",
    "Partimos por el original: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c8049e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7915"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "np.mean(cross_val_score(clf, X_std, y_sample, cv=5, scoring='accuracy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f89894",
   "metadata": {},
   "source": [
    "Ahora para el dataset transformado con autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "130b87ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6342"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf, X_auto, y_sample, cv=5, scoring='accuracy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478f28a",
   "metadata": {},
   "source": [
    "Y finalmente para la decomposicion con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63c6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X_std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e894a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6334"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf, X_pca, y_sample, cv=5, scoring='accuracy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d221e39a",
   "metadata": {},
   "source": [
    "### Mejores autoencoders, ¿podremos acercarnos más al acuracy del dataset original?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f1631",
   "metadata": {},
   "source": [
    "Un primer enfoque puede ser agregar más capas a la red neuronal. Acá abajo agregamos dos capas adicionales tanto en el encoder con el decoder: pasamos de *num_dimensiones* a 200 neuronas, luego a 50 y luego a 5, y lo mismo para decodificar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd2a19e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-10-15 17:50:38.300819: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 21952000 exceeds 10% of free system memory.\n",
      "2024-10-15 17:50:38.311283: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 21952000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8463 - val_loss: 5.4253\n",
      "Epoch 2/100\n",
      "\u001b[1m  1/219\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.7753"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7798 - val_loss: 5.3389\n",
      "Epoch 3/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7375 - val_loss: 5.2848\n",
      "Epoch 4/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6945 - val_loss: 5.2615\n",
      "Epoch 5/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6639 - val_loss: 5.2466\n",
      "Epoch 6/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6661 - val_loss: 5.2372\n",
      "Epoch 7/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6488 - val_loss: 5.2332\n",
      "Epoch 8/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6322 - val_loss: 5.2268\n",
      "Epoch 9/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6859 - val_loss: 5.2236\n",
      "Epoch 10/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6586 - val_loss: 5.2196\n",
      "Epoch 11/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6711 - val_loss: 5.2178\n",
      "Epoch 12/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6691 - val_loss: 5.2151\n",
      "Epoch 13/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6910 - val_loss: 5.2142\n",
      "Epoch 14/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6407 - val_loss: 5.2130\n",
      "Epoch 15/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6772 - val_loss: 5.2116\n",
      "Epoch 16/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6721 - val_loss: 5.2106\n",
      "Epoch 17/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6629 - val_loss: 5.2093\n",
      "Epoch 18/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6642 - val_loss: 5.2079\n",
      "Epoch 19/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6590 - val_loss: 5.2078\n",
      "Epoch 20/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6451 - val_loss: 5.2067\n",
      "Epoch 21/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6632 - val_loss: 5.2057\n",
      "Epoch 22/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6794 - val_loss: 5.2065\n",
      "Epoch 23/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6592 - val_loss: 5.2058\n",
      "Epoch 24/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6886 - val_loss: 5.2048\n",
      "Epoch 25/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6892 - val_loss: 5.2043\n",
      "Epoch 26/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6616 - val_loss: 5.2044\n",
      "Epoch 27/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6903 - val_loss: 5.2037\n",
      "Epoch 28/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6470 - val_loss: 5.2024\n",
      "Epoch 29/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6633 - val_loss: 5.2025\n",
      "Epoch 30/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6857 - val_loss: 5.2019\n",
      "Epoch 31/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6518 - val_loss: 5.2013\n",
      "Epoch 32/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6425 - val_loss: 5.2000\n",
      "Epoch 33/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7045 - val_loss: 5.1996\n",
      "Epoch 34/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6550 - val_loss: 5.2000\n",
      "Epoch 35/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6735 - val_loss: 5.1998\n",
      "Epoch 36/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6541 - val_loss: 5.2003\n",
      "Epoch 37/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6693 - val_loss: 5.1990\n",
      "Epoch 38/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7017 - val_loss: 5.1995\n",
      "Epoch 39/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6915 - val_loss: 5.1988\n",
      "Epoch 40/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6727 - val_loss: 5.1992\n",
      "Epoch 41/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6485 - val_loss: 5.1985\n",
      "Epoch 42/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6658 - val_loss: 5.1966\n",
      "Epoch 43/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6785 - val_loss: 5.1982\n",
      "Epoch 44/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6626 - val_loss: 5.1981\n",
      "Epoch 45/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6824 - val_loss: 5.1980\n",
      "Epoch 46/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6815 - val_loss: 5.1969\n",
      "Epoch 47/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6650 - val_loss: 5.1974\n",
      "Epoch 48/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6925 - val_loss: 5.1974\n",
      "Epoch 49/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6890 - val_loss: 5.1981\n",
      "Epoch 50/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6309 - val_loss: 5.1968\n",
      "Epoch 51/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6421 - val_loss: 5.1967\n",
      "Epoch 52/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6915 - val_loss: 5.1976\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step\n"
     ]
    }
   ],
   "source": [
    "### Modelo mas profundo\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(200, input_shape=[num_dimensiones]),\n",
    "    keras.layers.Dense(50),\n",
    "    keras.layers.Dense(5),\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(50, input_shape=[5]),\n",
    "    keras.layers.Dense(200),\n",
    "    keras.layers.Dense(num_dimensiones),\n",
    "])\n",
    "\n",
    "autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "autoencoder.compile(loss='mse', optimizer = keras.optimizers.SGD(learning_rate=0.1))\n",
    "\n",
    "history = autoencoder.fit(X_train_std,X_train_std, epochs=100,validation_data=(X_test_std,X_test_std),\n",
    "                         callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "                         \n",
    "X_auto_deep = encoder.predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d09a4b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6372"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf, X_auto_deep, y_sample, cv=5, scoring='accuracy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a2a35a",
   "metadata": {},
   "source": [
    "## Autoencoders con activación no lineal\n",
    "\n",
    "Un segundo enfoque, y lo más clásico en la práctica, es agregar una activación no lineal al final de cada capa. En este caso usamos la función Scaled Exponential Linear Unit (SELU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf77b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8797 - val_loss: 5.3075\n",
      "Epoch 2/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7767 - val_loss: 5.2573\n",
      "Epoch 3/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7545 - val_loss: 5.2256\n",
      "Epoch 4/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7375 - val_loss: 5.2075\n",
      "Epoch 5/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7338 - val_loss: 5.1994\n",
      "Epoch 6/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6852 - val_loss: 5.1946\n",
      "Epoch 7/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6835 - val_loss: 5.1910\n",
      "Epoch 8/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6928 - val_loss: 5.1893\n",
      "Epoch 9/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7050 - val_loss: 5.1874\n",
      "Epoch 10/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6828 - val_loss: 5.1856\n",
      "Epoch 11/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6741 - val_loss: 5.1850\n",
      "Epoch 12/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6761 - val_loss: 5.1835\n",
      "Epoch 13/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6563 - val_loss: 5.1819\n",
      "Epoch 14/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6699 - val_loss: 5.1805\n",
      "Epoch 15/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6698 - val_loss: 5.1782\n",
      "Epoch 16/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6719 - val_loss: 5.1751\n",
      "Epoch 17/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6610 - val_loss: 5.1732\n",
      "Epoch 18/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7052 - val_loss: 5.1703\n",
      "Epoch 19/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6724 - val_loss: 5.1657\n",
      "Epoch 20/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6582 - val_loss: 5.1625\n",
      "Epoch 21/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6087 - val_loss: 5.1592\n",
      "Epoch 22/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6336 - val_loss: 5.1550\n",
      "Epoch 23/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6627 - val_loss: 5.1517\n",
      "Epoch 24/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6730 - val_loss: 5.1491\n",
      "Epoch 25/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6610 - val_loss: 5.1457\n",
      "Epoch 26/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6476 - val_loss: 5.1437\n",
      "Epoch 27/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6416 - val_loss: 5.1409\n",
      "Epoch 28/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6094 - val_loss: 5.1384\n",
      "Epoch 29/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6504 - val_loss: 5.1364\n",
      "Epoch 30/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6206 - val_loss: 5.1349\n",
      "Epoch 31/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6516 - val_loss: 5.1313\n",
      "Epoch 32/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6321 - val_loss: 5.1304\n",
      "Epoch 33/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6505 - val_loss: 5.1273\n",
      "Epoch 34/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6018 - val_loss: 5.1250\n",
      "Epoch 35/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6046 - val_loss: 5.1239\n",
      "Epoch 36/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6215 - val_loss: 5.1223\n",
      "Epoch 37/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6240 - val_loss: 5.1205\n",
      "Epoch 38/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6290 - val_loss: 5.1200\n",
      "Epoch 39/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6010 - val_loss: 5.1198\n",
      "Epoch 40/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6109 - val_loss: 5.1182\n",
      "Epoch 41/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6138 - val_loss: 5.1168\n",
      "Epoch 42/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6053 - val_loss: 5.1153\n",
      "Epoch 43/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5807 - val_loss: 5.1142\n",
      "Epoch 44/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5634 - val_loss: 5.1128\n",
      "Epoch 45/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6034 - val_loss: 5.1115\n",
      "Epoch 46/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5903 - val_loss: 5.1124\n",
      "Epoch 47/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5707 - val_loss: 5.1090\n",
      "Epoch 48/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5711 - val_loss: 5.1146\n",
      "Epoch 49/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6041 - val_loss: 5.1110\n",
      "Epoch 50/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5952 - val_loss: 5.1132\n",
      "Epoch 51/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5747 - val_loss: 5.1073\n",
      "Epoch 52/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5764 - val_loss: 5.1104\n",
      "Epoch 53/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5785 - val_loss: 5.1106\n",
      "Epoch 54/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5627 - val_loss: 5.1054\n",
      "Epoch 55/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5743 - val_loss: 5.1050\n",
      "Epoch 56/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5681 - val_loss: 5.1078\n",
      "Epoch 57/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5547 - val_loss: 5.1055\n",
      "Epoch 58/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6042 - val_loss: 5.1053\n",
      "Epoch 59/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5510 - val_loss: 5.1017\n",
      "Epoch 60/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5916 - val_loss: 5.1022\n",
      "Epoch 61/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5802 - val_loss: 5.1036\n",
      "Epoch 62/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5337 - val_loss: 5.0977\n",
      "Epoch 63/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5408 - val_loss: 5.0995\n",
      "Epoch 64/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5505 - val_loss: 5.0941\n",
      "Epoch 65/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5386 - val_loss: 5.0890\n",
      "Epoch 66/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5421 - val_loss: 5.0928\n",
      "Epoch 67/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5464 - val_loss: 5.0943\n",
      "Epoch 68/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5205 - val_loss: 5.0908\n",
      "Epoch 69/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5445 - val_loss: 5.0961\n",
      "Epoch 70/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5332 - val_loss: 5.0892\n",
      "Epoch 71/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5400 - val_loss: 5.0908\n",
      "Epoch 72/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5235 - val_loss: 5.0882\n",
      "Epoch 73/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5442 - val_loss: 5.0798\n",
      "Epoch 74/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5293 - val_loss: 5.0785\n",
      "Epoch 75/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5297 - val_loss: 5.0885\n",
      "Epoch 76/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5381 - val_loss: 5.0802\n",
      "Epoch 77/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5153 - val_loss: 5.0830\n",
      "Epoch 78/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5280 - val_loss: 5.0735\n",
      "Epoch 79/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5568 - val_loss: 5.0682\n",
      "Epoch 80/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5019 - val_loss: 5.0655\n",
      "Epoch 81/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5558 - val_loss: 5.0671\n",
      "Epoch 82/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5216 - val_loss: 5.0606\n",
      "Epoch 83/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5095 - val_loss: 5.0536\n",
      "Epoch 84/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5164 - val_loss: 5.0623\n",
      "Epoch 85/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5153 - val_loss: 5.0426\n",
      "Epoch 86/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5037 - val_loss: 5.0378\n",
      "Epoch 87/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5120 - val_loss: 5.0385\n",
      "Epoch 88/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5108 - val_loss: 5.0417\n",
      "Epoch 89/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4945 - val_loss: 5.0144\n",
      "Epoch 90/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5271 - val_loss: 5.0297\n",
      "Epoch 91/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4998 - val_loss: 5.0078\n",
      "Epoch 92/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5223 - val_loss: 5.0071\n",
      "Epoch 93/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5319 - val_loss: 5.0319\n",
      "Epoch 94/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4989 - val_loss: 5.0065\n",
      "Epoch 95/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5164 - val_loss: 4.9899\n",
      "Epoch 96/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4971 - val_loss: 4.9816\n",
      "Epoch 97/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5012 - val_loss: 4.9684\n",
      "Epoch 98/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5046 - val_loss: 4.9809\n",
      "Epoch 99/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4725 - val_loss: 4.9565\n",
      "Epoch 100/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4907 - val_loss: 4.9580\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step\n"
     ]
    }
   ],
   "source": [
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(200, input_shape=[num_dimensiones],activation='selu'),\n",
    "    keras.layers.Dense(50,activation='selu'),\n",
    "    keras.layers.Dense(5,activation='selu'),\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(50, input_shape=[5],activation='selu'),\n",
    "    keras.layers.Dense(200,activation='selu'),\n",
    "    keras.layers.Dense(num_dimensiones,activation='selu'),\n",
    "])\n",
    "\n",
    "autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "autoencoder.compile(loss='mse', optimizer = keras.optimizers.SGD(learning_rate=0.1))\n",
    "\n",
    "history = autoencoder.fit(X_train_std,X_train_std, epochs=100,validation_data=(X_test_std,X_test_std),\n",
    "                         callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "                         \n",
    "X_auto_deep_relu = encoder.predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d3b7b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7186"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf, X_auto_deep_relu, y_sample, cv=5, scoring='accuracy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c5070",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Experimenta con distintos modelos que reduzcan a dos dimensiones. Visualiza y compara como lo hicimos en la tarea 3, con las reducciones con PCA y T-SNE, tomando como referencia las clases reales (los números que representan cada fila). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
